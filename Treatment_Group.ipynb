{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0ecf7f",
   "metadata": {},
   "source": [
    "# Finding the Treatment Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912856a",
   "metadata": {},
   "source": [
    "Pre requisites\n",
    "1. config.py in the config folder in the following format\n",
    "ACCESS_TOKEN = \"ghp_xxxx\"\n",
    "GITHUB_TOKEN = \"github_pat_xxxxx\"\n",
    "2. data folder where all data will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ecae2",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3f448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce46e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = config.ACCESS_TOKEN\n",
    "github_token = config.GITHUB_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b964f3",
   "metadata": {},
   "source": [
    "#### Download the example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fad7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists at data\\2023-04-01-15.json.gz. No need to download.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'data' folder exists\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "url = 'https://data.gharchive.org/2023-04-01-15.json.gz'\n",
    "file_path = os.path.join('data', '2023-04-01-15.json.gz')\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file already exists at {file_path}. No need to download.\")\n",
    "else:\n",
    "    response = requests.get(url, stream=True)\n",
    "    # Check if the request was successful (HTTP Status Code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Write the file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully and saved to {file_path}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch the file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25880f0",
   "metadata": {},
   "source": [
    "#### read it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fb9ff13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>actor</th>\n",
       "      <th>repo</th>\n",
       "      <th>payload</th>\n",
       "      <th>public</th>\n",
       "      <th>created_at</th>\n",
       "      <th>org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28137501092</td>\n",
       "      <td>PushEvent</td>\n",
       "      <td>{'id': 41898282, 'login': 'github-actions[bot]...</td>\n",
       "      <td>{'id': 568277185, 'name': 'stdlib-js/strided-b...</td>\n",
       "      <td>{'repository_id': 568277185, 'push_id': 131547...</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-01 15:00:00+00:00</td>\n",
       "      <td>{'id': 17805691, 'login': 'stdlib-js', 'gravat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28137501094</td>\n",
       "      <td>CreateEvent</td>\n",
       "      <td>{'id': 115239975, 'login': 'ishuduwal', 'displ...</td>\n",
       "      <td>{'id': 622248753, 'name': 'ishuduwal/personal-...</td>\n",
       "      <td>{'ref': 'main', 'ref_type': 'branch', 'master_...</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-01 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28137501097</td>\n",
       "      <td>CreateEvent</td>\n",
       "      <td>{'id': 50960481, 'login': 'bxbao87', 'display_...</td>\n",
       "      <td>{'id': 622248756, 'name': 'bxbao87/bloglist', ...</td>\n",
       "      <td>{'ref': 'main', 'ref_type': 'branch', 'master_...</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-01 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28137501098</td>\n",
       "      <td>PushEvent</td>\n",
       "      <td>{'id': 52915358, 'login': 'alwaz-shahid', 'dis...</td>\n",
       "      <td>{'id': 622201481, 'name': 'alwaz-shahid/extens...</td>\n",
       "      <td>{'repository_id': 622201481, 'push_id': 131547...</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-01 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28137501099</td>\n",
       "      <td>CreateEvent</td>\n",
       "      <td>{'id': 101326737, 'login': 'HOVADOVOLE', 'disp...</td>\n",
       "      <td>{'id': 622248605, 'name': 'HOVADOVOLE/Serial-P...</td>\n",
       "      <td>{'ref': 'main', 'ref_type': 'branch', 'master_...</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-01 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id         type  \\\n",
       "0  28137501092    PushEvent   \n",
       "1  28137501094  CreateEvent   \n",
       "2  28137501097  CreateEvent   \n",
       "3  28137501098    PushEvent   \n",
       "4  28137501099  CreateEvent   \n",
       "\n",
       "                                               actor  \\\n",
       "0  {'id': 41898282, 'login': 'github-actions[bot]...   \n",
       "1  {'id': 115239975, 'login': 'ishuduwal', 'displ...   \n",
       "2  {'id': 50960481, 'login': 'bxbao87', 'display_...   \n",
       "3  {'id': 52915358, 'login': 'alwaz-shahid', 'dis...   \n",
       "4  {'id': 101326737, 'login': 'HOVADOVOLE', 'disp...   \n",
       "\n",
       "                                                repo  \\\n",
       "0  {'id': 568277185, 'name': 'stdlib-js/strided-b...   \n",
       "1  {'id': 622248753, 'name': 'ishuduwal/personal-...   \n",
       "2  {'id': 622248756, 'name': 'bxbao87/bloglist', ...   \n",
       "3  {'id': 622201481, 'name': 'alwaz-shahid/extens...   \n",
       "4  {'id': 622248605, 'name': 'HOVADOVOLE/Serial-P...   \n",
       "\n",
       "                                             payload  public  \\\n",
       "0  {'repository_id': 568277185, 'push_id': 131547...    True   \n",
       "1  {'ref': 'main', 'ref_type': 'branch', 'master_...    True   \n",
       "2  {'ref': 'main', 'ref_type': 'branch', 'master_...    True   \n",
       "3  {'repository_id': 622201481, 'push_id': 131547...    True   \n",
       "4  {'ref': 'main', 'ref_type': 'branch', 'master_...    True   \n",
       "\n",
       "                 created_at                                                org  \n",
       "0 2023-04-01 15:00:00+00:00  {'id': 17805691, 'login': 'stdlib-js', 'gravat...  \n",
       "1 2023-04-01 15:00:00+00:00                                                NaN  \n",
       "2 2023-04-01 15:00:00+00:00                                                NaN  \n",
       "3 2023-04-01 15:00:00+00:00                                                NaN  \n",
       "4 2023-04-01 15:00:00+00:00                                                NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"data/2023-04-01-15.json.gz\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77a69d",
   "metadata": {},
   "source": [
    "## 1. Identifying user location based on their GitHub profiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed77c0",
   "metadata": {},
   "source": [
    "####  GraphQL for batch requests to fetch data for multiple users in one request instead of making a request for each user and constantly hitting the rate limit for the REST API\n",
    "1. GraphQL Test: Creates data subset, constructs/executes GraphQL query for user locations\n",
    "2. Italian Identification: Uses Italian keywords to filter and display Italian users\n",
    "- subset of 500 Users >> 3 Italians identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a8d4b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for user: github_actions_bot_\n",
      "No data for user: Ankityadavk\n",
      "No data for user: dependabot_bot_\n",
      "No data for user: shopify_bot_\n",
      "No data for user: renovate_bot_\n",
      "No data for user: pull_bot_\n",
      "No data for user: vercel_bot_\n",
      "No data for user: richardaeh\n",
      "No data for user: imgbot_bot_\n",
      "No data for user: treaty321\n",
      "No data for user: bakiamna99999999999\n",
      "No data for user: stale_bot_\n",
      "No data for user: cyousemi\n",
      "No data for user: netlify_bot_\n",
      "No data for user: GeneralAwareness\n",
      "No data for user: aws_connector_for_github_bot_\n",
      "No data for user: M_Lalaina\n"
     ]
    }
   ],
   "source": [
    "subset_df2 = df.head(500).copy()\n",
    "\n",
    "def sanitize_for_alias(username):\n",
    "    return ''.join(ch if ch.isalnum() else '_' for ch in username)\n",
    "\n",
    "logins = subset_df2['actor'].apply(lambda x: x['login']).tolist()\n",
    "\n",
    "# Filter out logins that start with a number\n",
    "logins = [login for login in logins if not login[0].isdigit()]\n",
    "\n",
    "# Construct GraphQL query\n",
    "query_parts = [f'''\n",
    "{sanitize_for_alias(login)}: user(login: \"{login}\") {{\n",
    "    location\n",
    "}}\n",
    "''' for login in logins]\n",
    "query = '{' + ''.join(query_parts) + '}'\n",
    "\n",
    "# Execute GraphQL query\n",
    "headers = {\n",
    "    'Authorization': 'bearer '+github_token,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.github.com/graphql', json={'query': query}, headers=headers)\n",
    "response_json = response.json()\n",
    "\n",
    "if 'data' not in response_json:\n",
    "    print(\"Error in response:\", response_json)\n",
    "    # Halt execution in a Jupyter notebook or similar environment\n",
    "    raise Exception(\"Data key missing from response\")\n",
    "\n",
    "data = response_json['data']\n",
    "\n",
    "# Update subset_df2 with fetched location data\n",
    "for login, user_data in data.items():\n",
    "    if user_data is None:\n",
    "        print(f\"No data for user: {login}\")\n",
    "        continue\n",
    "    location = user_data.get('location', None)\n",
    "    subset_df2.loc[subset_df2['actor'].apply(lambda x: x['login']) == login, 'actor'] = subset_df2['actor'].apply(\n",
    "        lambda x: {**x, 'location': location} if x['login'] == login else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb642600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# List of major cities in Italy and other possible indications of an Italian location\n",
    "italian_keywords = [\n",
    "    \"rome\", \"roma\", \"milan\", \"milano\", \"naples\", \"napoli\", \"turin\", \"torino\", \"palermo\", \n",
    "    \"genoa\", \"genova\", \"bologna\", \"florence\", \"firenze\", \"venice\", \"venezia\", \"verona\", \n",
    "    \"cagliari\", \"parma\", \"ferrara\", \"treviso\", \"padua\", \"padova\", \"trieste\", \"taranto\", \n",
    "    \"brescia\", \"prato\", \"modena\", \"reggio\", \"calabria\", \"emilia\", \"perugia\", \"livorno\", \n",
    "    \"ravenna\", \"foggia\", \"rimini\", \"salerno\", \"sassari\", \"latina\", \"giugliano\", \"tuscany\", \n",
    "    \"toscana\", \"sicily\", \"sicilia\", \"sardinia\", \"sardegna\", \"lombardy\", \"lombardia\", \"piedmont\", \n",
    "    \"piemonte\", \"liguria\", \"calabria\", \"umbria\", \"marche\", \"abruzzo\", \"italy\", \"italia\"\n",
    "]\n",
    "\n",
    "def is_italian_location(location):\n",
    "    if not location:\n",
    "        return False\n",
    "    location = location.lower()\n",
    "    if any(keyword in location for keyword in italian_keywords):\n",
    "        return True\n",
    "    # Using fuzzy matching to account for typos\n",
    "    closest_match, score = process.extractOne(location, italian_keywords)\n",
    "    return score > 80\n",
    "\n",
    "# Filter out rows with Italian locations\n",
    "non_italian_df = subset_df2[~subset_df2['actor'].apply(lambda x: is_italian_location(x.get('location')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a630fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>login</th>\n",
       "      <th>display_login</th>\n",
       "      <th>gravatar_id</th>\n",
       "      <th>url</th>\n",
       "      <th>avatar_url</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16925025</td>\n",
       "      <td>maffo102</td>\n",
       "      <td>maffo102</td>\n",
       "      <td></td>\n",
       "      <td>https://api.github.com/users/maffo102</td>\n",
       "      <td>https://avatars.githubusercontent.com/u/16925025?</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30238962</td>\n",
       "      <td>merkleID</td>\n",
       "      <td>merkleID</td>\n",
       "      <td></td>\n",
       "      <td>https://api.github.com/users/merkleID</td>\n",
       "      <td>https://avatars.githubusercontent.com/u/30238962?</td>\n",
       "      <td>milan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117077787</td>\n",
       "      <td>Nelexiad</td>\n",
       "      <td>Nelexiad</td>\n",
       "      <td></td>\n",
       "      <td>https://api.github.com/users/Nelexiad</td>\n",
       "      <td>https://avatars.githubusercontent.com/u/117077...</td>\n",
       "      <td>Palermo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     login display_login gravatar_id  \\\n",
       "0   16925025  maffo102      maffo102               \n",
       "1   30238962  merkleID      merkleID               \n",
       "2  117077787  Nelexiad      Nelexiad               \n",
       "\n",
       "                                     url  \\\n",
       "0  https://api.github.com/users/maffo102   \n",
       "1  https://api.github.com/users/merkleID   \n",
       "2  https://api.github.com/users/Nelexiad   \n",
       "\n",
       "                                          avatar_url location  \n",
       "0  https://avatars.githubusercontent.com/u/16925025?    Italy  \n",
       "1  https://avatars.githubusercontent.com/u/30238962?    milan  \n",
       "2  https://avatars.githubusercontent.com/u/117077...  Palermo  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter in rows with Italian locations\n",
    "italian_df = subset_df2[subset_df2['actor'].apply(lambda x: is_italian_location(x.get('location')))]\n",
    "\n",
    "# Flatten the 'actor' column from the italian_df\n",
    "flattened_italian_actor_df = pd.json_normalize(italian_df['actor'])\n",
    "\n",
    "# Display the flattened 'actor' column\n",
    "flattened_italian_actor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60892f7e",
   "metadata": {},
   "source": [
    "## Email Analysis\n",
    "I added a limit of 100 \"df['actor'][:100]\"\n",
    "\n",
    "With this limit I didnt find any email :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73357539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying user github-actions[bot]: Could not resolve to a User with the login of 'github-actions[bot]'.\n",
      "No email found for user github-actions[bot]\n",
      "No email found for user ishuduwal\n",
      "No email found for user bxbao87\n",
      "No email found for user alwaz-shahid\n",
      "No email found for user HOVADOVOLE\n",
      "No email found for user FlorBera\n",
      "No email found for user LombiqBot\n",
      "No email found for user thuanowa\n",
      "Error querying user Ankityadavk: Could not resolve to a User with the login of 'Ankityadavk'.\n",
      "No email found for user Ankityadavk\n",
      "No email found for user gokaysatir\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user pultho\n",
      "No email found for user rswnsyh\n",
      "Error querying user shopify[bot]: Could not resolve to a User with the login of 'shopify[bot]'.\n",
      "No email found for user shopify[bot]\n",
      "No email found for user Rindraniaina-28\n",
      "Error querying user renovate[bot]: Could not resolve to a User with the login of 'renovate[bot]'.\n",
      "No email found for user renovate[bot]\n",
      "No email found for user Kuciuks\n",
      "No email found for user kyleronc\n",
      "Error querying user github-actions[bot]: Could not resolve to a User with the login of 'github-actions[bot]'.\n",
      "No email found for user github-actions[bot]\n",
      "No email found for user Arnult\n",
      "No email found for user gennady-bars\n",
      "No email found for user Nurikan-sketch\n",
      "No email found for user toonchavez8\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user pull[bot]: Could not resolve to a User with the login of 'pull[bot]'.\n",
      "No email found for user pull[bot]\n",
      "No email found for user radoering\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user Pediastrum\n",
      "No email found for user Rvp888\n",
      "No email found for user keshasha\n",
      "No email found for user aws-aemilia-hkg\n",
      "No email found for user elf8016\n",
      "No email found for user gennady-bars\n",
      "No email found for user AerixTenderly\n",
      "Error querying user renovate[bot]: Could not resolve to a User with the login of 'renovate[bot]'.\n",
      "No email found for user renovate[bot]\n",
      "No email found for user Tienisto\n",
      "No email found for user Tesabegs\n",
      "No email found for user renovate-bot\n",
      "No email found for user jhonboy121\n",
      "Error querying user renovate[bot]: Could not resolve to a User with the login of 'renovate[bot]'.\n",
      "No email found for user renovate[bot]\n",
      "No email found for user Zhytou\n",
      "No email found for user wizardofzos\n",
      "No email found for user harpreet1602\n",
      "No email found for user AerixTenderly\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user xc1y46\n",
      "No email found for user kitaken1118\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user wukibaka\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user vercel[bot]: Could not resolve to a User with the login of 'vercel[bot]'.\n",
      "No email found for user vercel[bot]\n",
      "No email found for user guilherme4garcia\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user shyjnnn\n",
      "Error querying user github-actions[bot]: Could not resolve to a User with the login of 'github-actions[bot]'.\n",
      "No email found for user github-actions[bot]\n",
      "No email found for user juju812\n",
      "Error querying user github-actions[bot]: Could not resolve to a User with the login of 'github-actions[bot]'.\n",
      "No email found for user github-actions[bot]\n",
      "No email found for user Tienisto\n",
      "No email found for user 5af7a301\n",
      "No email found for user JumppanenTomi\n",
      "No email found for user AyushG2026\n",
      "No email found for user JusticeIsreal\n",
      "No email found for user avanhoveln-BITCO\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user hackernews-archive\n",
      "No email found for user akhilya\n",
      "No email found for user sk-ummesalma\n",
      "Error querying user vercel[bot]: Could not resolve to a User with the login of 'vercel[bot]'.\n",
      "No email found for user vercel[bot]\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user himyne\n",
      "No email found for user 196code-gray\n",
      "No email found for user leomp12\n",
      "Error querying user dependabot[bot]: Could not resolve to a User with the login of 'dependabot[bot]'.\n",
      "No email found for user dependabot[bot]\n",
      "No email found for user mo9a7i\n",
      "Error querying user renovate[bot]: Could not resolve to a User with the login of 'renovate[bot]'.\n",
      "No email found for user renovate[bot]\n",
      "No email found for user yoctottaops\n",
      "No email found for user elycisjasa123\n",
      "No email found for user dev-kian\n",
      "No email found for user Naapperas\n",
      "No email found for user code423n4\n",
      "No email found for user direwolf-github\n",
      "No email found for user JimmyMetz3\n",
      "No email found for user kotyara29\n",
      "No email found for user JumppanenTomi\n",
      "No email found for user Sangyups\n",
      "No email found for user hai000\n",
      "No email found for user matzssue\n",
      "No email found for user elayne-nik\n",
      "No email found for user Hirozy\n",
      "Error querying user renovate[bot]: Could not resolve to a User with the login of 'renovate[bot]'.\n",
      "No email found for user renovate[bot]\n",
      "Error querying user vercel[bot]: Could not resolve to a User with the login of 'vercel[bot]'.\n",
      "No email found for user vercel[bot]\n",
      "No email found for user igabriel-gb\n",
      "No email found for user AyushG2026\n",
      "No email found for user direwolf-github\n",
      "No email found for user N30E17\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from graphqlclient import GraphQLClient\n",
    "from config import config\n",
    "\n",
    "def get_client():\n",
    "    client = GraphQLClient('https://api.github.com/graphql')\n",
    "    client.inject_token(f'Bearer {github_token}')\n",
    "    return client\n",
    "\n",
    "def execute_query(client, query, variables=None):\n",
    "    try:\n",
    "        result = client.execute(query, variables)\n",
    "        return json.loads(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_user_email(client, username):\n",
    "    query = \"\"\"\n",
    "    query($username: String!) {\n",
    "      user(login: $username) {\n",
    "        email\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    variables = {\"username\": username}\n",
    "    \n",
    "    response_json = execute_query(client, query, variables)\n",
    "    \n",
    "    if response_json and \"errors\" in response_json:\n",
    "        error_message = response_json.get(\"errors\")[0].get(\"message\")\n",
    "        print(f\"Error querying user {username}: {error_message}\")\n",
    "        return None\n",
    "    \n",
    "    return response_json[\"data\"][\"user\"][\"email\"] if response_json else None\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    client = get_client()\n",
    "    emails = []\n",
    "\n",
    "    for row in df['actor'][:100]:\n",
    "        login = row.get('login')\n",
    "        if login:\n",
    "            email = get_user_email(client, login)\n",
    "            if email:\n",
    "                emails.append(email)\n",
    "            else:\n",
    "                print(f\"No email found for user {login}\")\n",
    "\n",
    "    print(emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c3fb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd0fc1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails ending with '.it':\n",
      "Number of emails ending with '.it': 0\n"
     ]
    }
   ],
   "source": [
    "it_emails = []\n",
    "\n",
    "for email in emails:\n",
    "    if email.endswith(\".ca\"):\n",
    "        it_emails.append(email)\n",
    "\n",
    "print(\"Emails ending with '.it':\")\n",
    "for it_email in it_emails:\n",
    "    print(it_email)\n",
    "\n",
    "count_it_emails = len(it_emails)\n",
    "print(f\"Number of emails ending with '.it': {count_it_emails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11543b26",
   "metadata": {},
   "source": [
    "## 6. Analyzing org descriptions\n",
    "Use GraphQL to get more information at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a35117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stdlib-js' 'Lombiq' 'CollaboraOnline' ... 'nocalhost' 'linnovate'\n",
      " 'coreweave']\n"
     ]
    }
   ],
   "source": [
    "unique_orgs = pd.Series(df['org'].dropna().apply(lambda x: x['login'])).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a70804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying organization FZM-Technologies: Could not resolve to an Organization with the login of 'FZM-Technologies'.\n",
      "Error querying organization chainparrot: Could not resolve to an Organization with the login of 'chainparrot'.\n",
      "Error querying organization playing-ground: Could not resolve to an Organization with the login of 'playing-ground'.\n",
      "Error querying organization keptn-demo-live: Could not resolve to an Organization with the login of 'keptn-demo-live'.\n",
      "Error querying organization Strada1: Could not resolve to an Organization with the login of 'Strada1'.\n",
      "Error querying organization iruoy-nl: Could not resolve to an Organization with the login of 'iruoy-nl'.\n",
      "Error querying organization eras-fyi: Could not resolve to an Organization with the login of 'eras-fyi'.\n",
      "Error querying organization DataFest-2023-Algo-Rhythms: Could not resolve to an Organization with the login of 'DataFest-2023-Algo-Rhythms'.\n",
      "Error querying organization codesquad2023-fe-algorithm: Could not resolve to an Organization with the login of 'codesquad2023-fe-algorithm'.\n",
      "Error querying organization Polls-team: Could not resolve to an Organization with the login of 'Polls-team'.\n",
      "Error querying organization coders-app: Could not resolve to an Organization with the login of 'coders-app'.\n",
      "Error querying organization xManager-App: Could not resolve to an Organization with the login of 'xManager-App'.\n",
      "Error querying organization new-ai-company: Could not resolve to an Organization with the login of 'new-ai-company'.\n",
      "Error querying organization CodeCommun-co: Could not resolve to an Organization with the login of 'CodeCommun-co'.\n",
      "Error querying organization MAJigsaws-Storage: Could not resolve to an Organization with the login of 'MAJigsaws-Storage'.\n",
      "                  name                                        description\n",
      "0   dalva-reconfeccoes                                                   \n",
      "1             lokalise  Translation platform for developers. Upload la...\n",
      "2  release-engineering                                                   \n",
      "3              outline  We're building an open source collaborative kn...\n",
      "4                jfrog                                                   \n"
     ]
    }
   ],
   "source": [
    "# List of unique organization logins obtained from your DataFrame\n",
    "unique_org_list = unique_orgs[100:900]\n",
    "\n",
    "# Setup GraphQL Client\n",
    "client = GraphQLClient('https://api.github.com/graphql')\n",
    "client.inject_token(f'Bearer {github_token}')\n",
    "\n",
    "# Define GraphQL Query\n",
    "query = \"\"\"\n",
    "query($login: String!) {\n",
    "  organization(login: $login) {\n",
    "    login\n",
    "    description\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# List to store data for the new DataFrame\n",
    "data = []\n",
    "\n",
    "# Loop through each unique organization login\n",
    "for org_login in unique_org_list:\n",
    "    try:\n",
    "        # Define variables for the query\n",
    "        variables = {\"login\": org_login}\n",
    "        \n",
    "        # Execute the GraphQL query\n",
    "        result = client.execute(query, variables)\n",
    "        response_json = json.loads(result)\n",
    "        \n",
    "        # Check for errors in the response\n",
    "        if \"errors\" in response_json:\n",
    "            error_message = response_json['errors'][0]['message']\n",
    "            print(f\"Error querying organization {org_login}: {error_message}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract the relevant data\n",
    "        org_data = response_json['data']['organization']\n",
    "        data.append([org_data['login'], org_data['description']])\n",
    "        \n",
    "    except Exception as err:\n",
    "        # Handle potential errors\n",
    "        print(f\"An error occurred for {org_login}: {err}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_orgs = pd.DataFrame(data, columns=['name', 'description'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_orgs.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ccdb6",
   "metadata": {},
   "source": [
    "### Lingua\n",
    "https://github.com/pemistahl/lingua-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5d276ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>polito-WA1-AW1-2023</td>\n",
       "      <td>Courses at Politecnico di Torino - Academic Ye...</td>\n",
       "      <td>ITALIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>MOVIECORD</td>\n",
       "      <td>ðŸŽ¬ Movie | Film Festival | OTT</td>\n",
       "      <td>ITALIAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                        description  \\\n",
       "305  polito-WA1-AW1-2023  Courses at Politecnico di Torino - Academic Ye...   \n",
       "605            MOVIECORD                      ðŸŽ¬ Movie | Film Festival | OTT   \n",
       "\n",
       "    language  \n",
       "305  ITALIAN  \n",
       "605  ITALIAN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install lingua-language-detector\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import pandas as pd\n",
    "\n",
    "df_lingua = df_orgs\n",
    "\n",
    "# Available languages\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH, Language.ITALIAN]\n",
    "\n",
    "# Build the language detector\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detector.detect_language_of(text).name\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Apply the function to the 'description' column and store the result in a new column 'language'\n",
    "df_lingua['language'] = df_lingua['description'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_lingua[df_lingua[\"language\"]==\"ITALIAN\"].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc165dec",
   "metadata": {},
   "source": [
    "### Langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28c45ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, description, language]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "df_langdetect = df_orgs\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Apply the function to the 'description' column and store the result in a new column 'language'\n",
    "df_langdetect['language'] = df_langdetect['description'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_langdetect[df_langdetect[\"language\"] == \"ITALIAN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cf6ca",
   "metadata": {},
   "source": [
    "## 7. Italian Open Source Licences\n",
    "Problem is, that the authors are italian but not living in Italy!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fa598b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2Ami',\n",
       " 'Apivault',\n",
       " 'Arduino',\n",
       " 'Arduino Desk Weatherstation',\n",
       " 'Argon',\n",
       " 'Autocannon',\n",
       " 'Awesome Italia Open Source',\n",
       " 'Bootstrap Italia',\n",
       " 'Breathly',\n",
       " 'Cache Candidate']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "def get_and_process_github_readme(url, filename, token):\n",
    "    \"\"\"\n",
    "    Download the README.md from GitHub, save it, extract and return the names of repositories.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to the raw README.md on GitHub.\n",
    "        filename (str): The name to save the README.md as.\n",
    "        token (str): Your GitHub API token.\n",
    "        \n",
    "    Returns:\n",
    "        list: Extracted repository names.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3.raw\"\n",
    "    }\n",
    "    \n",
    "    # Step 1: Download the README.md from GitHub\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        readme_content = response.text\n",
    "        \n",
    "        # Step 2: Process the saved file to extract repository names\n",
    "        name_pattern = re.compile(r'\\|\\s*(?:\\[(?P<link_name>.+?)\\]\\(.+?\\)|(?P<plain_name>[^\\|\\[\\]]+))\\s*\\|')\n",
    "        names = []\n",
    "        for line in readme_content.split('\\n'):\n",
    "            match = name_pattern.search(line)\n",
    "            if match:\n",
    "                name = (match.group('link_name') or match.group('plain_name')).strip()\n",
    "                if name and name.lower() not in [\"name\", \"----\", \"stack\", \"description\"]:\n",
    "                    names.append(name)\n",
    "        return names\n",
    "    else:\n",
    "        raise ConnectionError(f\"Failed to fetch README.md, status code: {response.status_code}\")\n",
    "\n",
    "# URL to the raw README.md on GitHub\n",
    "readme_url = \"https://github.com/italia-opensource/awesome-italia-opensource/raw/main/awesome/opensource/README.md\"\n",
    "\n",
    "# Filename to save the README.md as\n",
    "save_filename = \"github_readme.md\"\n",
    "\n",
    "# Execute the function and display the first 10 repository names\n",
    "Italian_OS_projects = get_and_process_github_readme(readme_url, save_filename, github_token)[1:]\n",
    "Italian_OS_projects[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6115e",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45a951",
   "metadata": {},
   "source": [
    "### downloading data for multiple days/hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b87db6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import List, Union\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "BASE_URL = \"https://data.gharchive.org\"\n",
    "\n",
    "def ensure_directory_exists(dir_name: str) -> None:\n",
    "    \"\"\"Ensure the specified directory exists.\"\"\"\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download file and save it to the specified path.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            logging.warning(f\"File not found: {url}\")\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch {url}: {str(e)}\")\n",
    "\n",
    "def load_or_fetch_data(file_path: str, url: str) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"Load data from file or fetch from URL if not exists.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        logging.info(f\"File {file_path} already exists. Loading data.\")\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as gz:\n",
    "            return pd.read_json(gz, lines=True)\n",
    "    else:\n",
    "        logging.info(f\"File {file_path} not exists. Downloading from {url}\")\n",
    "        download_file(url, file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as gz:\n",
    "                return pd.read_json(gz, lines=True)\n",
    "    return None\n",
    "\n",
    "def download_gh_archive(start_day: int, end_day: int, \n",
    "                        start_hour: int=0, end_hour: int=23,\n",
    "                        base_url: str=BASE_URL, data_dir: str='data') -> pd.DataFrame:\n",
    "    \"\"\"Download GitHub archive data for specified days and hours.\"\"\"\n",
    "    dfs = []\n",
    "    ensure_directory_exists(data_dir)\n",
    "    \n",
    "    for day in range(start_day, end_day + 1):\n",
    "        for hour in range(start_hour, end_hour + 1):\n",
    "            filename = f\"2023-04-{day:02d}-{hour}.json.gz\"\n",
    "            url = f\"{base_url}/{filename}\"\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            \n",
    "            df = load_or_fetch_data(file_path, url)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "334e1fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:File data\\2023-04-01-10.json.gz already exists. Loading data.\n",
      "INFO:root:File data\\2023-04-01-11.json.gz already exists. Loading data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m end_hour \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(end_date_time\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Download data for the specified time frame\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df_big \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_gh_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_hour\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_hour\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Record the end time and calculate the elapsed time\u001b[39;00m\n\u001b[0;32m     16\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[59], line 58\u001b[0m, in \u001b[0;36mdownload_gh_archive\u001b[1;34m(start_day, end_day, start_hour, end_hour, base_url, data_dir)\u001b[0m\n\u001b[0;32m     55\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, filename)\n\u001b[1;32m---> 58\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "Cell \u001b[1;32mIn[59], line 36\u001b[0m, in \u001b[0;36mload_or_fetch_data\u001b[1;34m(file_path, url)\u001b[0m\n\u001b[0;32m     34\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists. Loading data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m gz:\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not exists. Downloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\io\\json\\_json.py:757\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[1;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\io\\json\\_json.py:913\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    911\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    912\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 913\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\io\\json\\_json.py:937\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    935\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 937\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\io\\json\\_json.py:1064\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\io\\json\\_json.py:1320\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1317\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1324\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1325\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1327\u001b[0m     }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adpro\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m--> 664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaskedArray\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;66;03m# masked recarray\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "#Define the start and end date-time\n",
    "start_date_time = \"2023-04-01 10\"  # Format: \"YYYY-MM-DD HH\"\n",
    "end_date_time = \"2023-04-01 12\"    # Format: \"YYYY-MM-DD HH\"\n",
    "# Extract day and hour from the date-time strings\n",
    "start_day = int(start_date_time.split(\"-\")[2].split()[0])\n",
    "start_hour = int(start_date_time.split()[1])\n",
    "end_day = int(end_date_time.split(\"-\")[2].split()[0])\n",
    "end_hour = int(end_date_time.split()[1])\n",
    "\n",
    "# Download data for the specified time frame\n",
    "df_big = download_gh_archive(start_day, end_day, start_hour, end_hour)\n",
    "\n",
    "# Record the end time and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Data from {start_date_time} to {end_date_time} loaded into DataFrame!\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a9485",
   "metadata": {},
   "source": [
    "### Getting rid of Github Bot commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56206ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[~df['actor'].apply(lambda x: x.get('login').endswith('[bot]'))]\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
