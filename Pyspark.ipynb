{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0ecf7f",
   "metadata": {},
   "source": [
    "# Finding the Treatment Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912856a",
   "metadata": {},
   "source": [
    "Pre requisites\n",
    "1. config.py in the config folder in the following format\n",
    "ACCESS_TOKEN = \"ghp_xxxx\"\n",
    "GITHUB_TOKEN = \"github_pat_xxxxx\"\n",
    "2. data folder where all data will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ecae2",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47f5c8cd-de5f-4517-9a9d-2c520904ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from config import config\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "from typing import List, Union\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ce46e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = config.ACCESS_TOKEN\n",
    "github_token = config.GITHUB_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a13d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\mundt\\\\anaconda3\\\\envs\\\\work_project\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\mundt\\\\anaconda3\\\\envs\\\\work_project\\\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d30fcbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "816d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "## !!!! CHANGE THE MEMORY FOR THE FINAL MACHINE !!!!!\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"LargeJSONProcessing\") \\\n",
    "   .config(\"spark.driver.memory\", \"6g\") \\\n",
    "   .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b964f3",
   "metadata": {},
   "source": [
    "#### Download the example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fad7092",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists at data\\2023-04-01-15.json.gz. No need to download.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'data' folder exists\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "url = 'https://data.gharchive.org/2023-04-01-15.json.gz'\n",
    "file_path = os.path.join('data', '2023-04-01-15.json.gz')\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file already exists at {file_path}. No need to download.\")\n",
    "else:\n",
    "    response = requests.get(url, stream=True)\n",
    "    # Check if the request was successful (HTTP Status Code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Write the file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully and saved to {file_path}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch the file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25880f0",
   "metadata": {},
   "source": [
    "#### read in only the example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb9ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+------+--------------------+-----------+\n",
      "|               actor|          created_at|         id|                 org|             payload|public|                repo|       type|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+------+--------------------+-----------+\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501092|{https://avatars....|{NULL, e743bdd287...|  true|{568277185, stdli...|  PushEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501094|                NULL|{NULL, NULL, NULL...|  true|{622248753, ishud...|CreateEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501097|                NULL|{NULL, NULL, NULL...|  true|{622248756, bxbao...|CreateEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501098|                NULL|{NULL, 0233f4a6e9...|  true|{622201481, alwaz...|  PushEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501099|                NULL|{NULL, NULL, NULL...|  true|{622248605, HOVAD...|CreateEvent|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data using Spark\n",
    "df_spark = spark.read.json(file_path)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807bec7",
   "metadata": {},
   "source": [
    "### Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87db6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "BASE_URL = \"https://data.gharchive.org\"\n",
    "\n",
    "def ensure_directory_exists(dir_name: str) -> None:\n",
    "    \"\"\"Ensure the specified directory exists.\"\"\"\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download file and save it to the specified path.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            logging.warning(f\"File not found: {url}\")\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch {url}: {str(e)}\")\n",
    "\n",
    "def load_or_fetch_data(file_path: str, url: str) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"Load data from file or fetch from URL if not exists.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        logging.info(f\"File {file_path} already exists. Loading data.\")\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as gz:\n",
    "            return pd.read_json(gz, lines=True)\n",
    "    else:\n",
    "        logging.info(f\"File {file_path} not exists. Downloading from {url}\")\n",
    "        download_file(url, file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as gz:\n",
    "                return pd.read_json(gz, lines=True)\n",
    "    return None\n",
    "\n",
    "def download_gh_archive(start_day: int, end_day: int, \n",
    "                        start_hour: int=0, end_hour: int=23,\n",
    "                        base_url: str=BASE_URL, data_dir: str='data') -> pd.DataFrame:\n",
    "    \"\"\"Download GitHub archive data for specified days and hours.\"\"\"\n",
    "    dfs = []\n",
    "    ensure_directory_exists(data_dir)\n",
    "    \n",
    "    for day in range(start_day, end_day + 1):\n",
    "        for hour in range(start_hour, end_hour + 1):\n",
    "            filename = f\"2023-04-{day:02d}-{hour}.json.gz\"\n",
    "            url = f\"{base_url}/{filename}\"\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            \n",
    "            df = load_or_fetch_data(file_path, url)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:File data\\2023-04-01-1.json.gz not exists. Downloading from https://data.gharchive.org/2023-04-01-1.json.gz\n",
      "INFO:root:File data\\2023-04-01-2.json.gz not exists. Downloading from https://data.gharchive.org/2023-04-01-2.json.gz\n",
      "INFO:root:File data\\2023-04-01-3.json.gz not exists. Downloading from https://data.gharchive.org/2023-04-01-3.json.gz\n",
      "INFO:root:File data\\2023-04-01-4.json.gz not exists. Downloading from https://data.gharchive.org/2023-04-01-4.json.gz\n",
      "INFO:root:File data\\2023-04-01-5.json.gz not exists. Downloading from https://data.gharchive.org/2023-04-01-5.json.gz\n",
      "INFO:root:File data\\2023-04-01-6.json.gz not exists. Downloading from https://data.gharchive.org/2023-04-01-6.json.gz\n"
     ]
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "#Define the start and end date-time\n",
    "start_date_time = \"2023-04-01 01\"  # Format: \"YYYY-MM-DD HH\"\n",
    "end_date_time = \"2023-04-01 12\"    # Format: \"YYYY-MM-DD HH\"\n",
    "# Extract day and hour from the date-time strings\n",
    "start_day = int(start_date_time.split(\"-\")[2].split()[0])\n",
    "start_hour = int(start_date_time.split()[1])\n",
    "end_day = int(end_date_time.split(\"-\")[2].split()[0])\n",
    "end_hour = int(end_date_time.split()[1])\n",
    "\n",
    "# Download data for the specified time frame\n",
    "df_big = download_gh_archive(start_day, end_day, start_hour, end_hour)\n",
    "\n",
    "# Record the end time and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Data from {start_date_time} to {end_date_time} loaded into DataFrame!\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6a8c2",
   "metadata": {},
   "source": [
    "### Read in all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "499712bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+------+--------------------+-----------+\n",
      "|               actor|          created_at|         id|                 org|             payload|public|                repo|       type|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+------+--------------------+-----------+\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501092|{https://avatars....|{NULL, e743bdd287...|  true|{568277185, stdli...|  PushEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501094|                NULL|{NULL, NULL, NULL...|  true|{622248753, ishud...|CreateEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501097|                NULL|{NULL, NULL, NULL...|  true|{622248756, bxbao...|CreateEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501098|                NULL|{NULL, 0233f4a6e9...|  true|{622201481, alwaz...|  PushEvent|\n",
      "|{https://avatars....|2023-04-01T15:00:00Z|28137501099|                NULL|{NULL, NULL, NULL...|  true|{622248605, HOVAD...|CreateEvent|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the files\n",
    "data_dir = 'data'\n",
    "# List all files that match the pattern\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('2023') and f.endswith('.json.gz')]\n",
    "\n",
    "# Read and concatenate the files using Spark\n",
    "df_spark = spark.read.json(files)\n",
    "\n",
    "# Optional: If you're planning on performing multiple operations on the DataFrame, cache it\n",
    "# df_spark.cache()\n",
    "\n",
    "# Display the first few rows as a sample\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77a69d",
   "metadata": {},
   "source": [
    "## 1. Identifying user location based on their GitHub profiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed77c0",
   "metadata": {},
   "source": [
    "####  GraphQL for batch requests to fetch data for multiple users in one request instead of making a request for each user and constantly hitting the rate limit for the REST API\n",
    "1. GraphQL Test: Creates data subset, constructs/executes GraphQL query for user locations\n",
    "2. Italian Identification: Uses Italian keywords to filter and display Italian users\n",
    "- subset of 500 Users >> 3 Italians identified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ebaf3",
   "metadata": {},
   "source": [
    "#### Step 1: Extract Unique Logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d96cf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter out logins that start with a digit and extract unique logins\n",
    "logins_df = df_spark.filter(~col('actor.login').substr(1, 1).rlike('[0-9]')) \\\n",
    "                    .select('actor.login') \\\n",
    "                    .distinct()\n",
    "\n",
    "# Now collect the unique logins to the driver (this should be manageable since they're unique)\n",
    "logins = [row['login'] for row in logins_df.collect()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56d295",
   "metadata": {},
   "source": [
    "#### Step 2: Batched GraphQL Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcb66a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def sanitize_for_alias(username):\n",
    "    return ''.join(ch if ch.isalnum() else '_' for ch in username)\n",
    "\n",
    "def construct_query(logins):\n",
    "    query_parts = [\n",
    "        f'''\n",
    "        {sanitize_for_alias(login)}: user(login: \"{login}\") {{\n",
    "            location\n",
    "        }}\n",
    "        ''' for login in logins\n",
    "    ]\n",
    "    return '{' + ''.join(query_parts) + '}'\n",
    "\n",
    "def fetch_data(query, github_token):\n",
    "    headers = {\n",
    "        'Authorization': 'bearer ' + github_token,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post('https://api.github.com/graphql', json={'query': query}, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    response_json = response.json()\n",
    "    \n",
    "    if 'data' not in response_json:\n",
    "        raise Exception(\"Data key missing from response:\", response_json)\n",
    "    return response_json['data']\n",
    "\n",
    "def batched_fetch_data(logins, batch_size, github_token):\n",
    "    all_data = {}\n",
    "    \n",
    "    for i in range(0, len(logins), batch_size):\n",
    "        batch = logins[i:i + batch_size]\n",
    "        query = construct_query(batch)\n",
    "        data = fetch_data(query, github_token)\n",
    "        \n",
    "        # Update the results dictionary. The response data will have keys based on the sanitized alias.\n",
    "        # We want to map it back to the original login for simplicity.\n",
    "        for login in batch:\n",
    "            alias = sanitize_for_alias(login)\n",
    "            if alias in data:\n",
    "                user_data = data[alias]\n",
    "                # Using the original login here\n",
    "                all_data[login] = user_data\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Adjusting the batch size to 5000 as you mentioned\n",
    "batch_size = 2000\n",
    "all_user_data = batched_fetch_data(logins, batch_size, github_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3bfd5",
   "metadata": {},
   "source": [
    "### Step 3: Create a DataFrame from GraphQL Results and Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bb92453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              login|            location|\n",
      "+-------------------+--------------------+\n",
      "|            iMariee|                NULL|\n",
      "|        edu-rinaldi|              Berlin|\n",
      "|Chaitanyashrimali21|                NULL|\n",
      "|        shubhamkr83|           New Delhi|\n",
      "|         FiresoftPH|                NULL|\n",
      "|            Pamavoc|               Paris|\n",
      "|            Ituking|                NULL|\n",
      "|               cpba|                NULL|\n",
      "|        Trishna1234|             Kolkata|\n",
      "|         Pablotramp|                NULL|\n",
      "|            dev0652|Ukraine, best cou...|\n",
      "|           Hritik95|                NULL|\n",
      "|        YannickMath|                NULL|\n",
      "|         Nanonikich|                NULL|\n",
      "|  Sneha-Mittal88293|Bari , Dholpur (R...|\n",
      "|           shweepps|                NULL|\n",
      "|             vr33ni|                NULL|\n",
      "|         FruiteePro|               Earth|\n",
      "|      adviti-mishra|                NULL|\n",
      "|             gfmota|       São Paulo -SP|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the user data dictionary to a list of Rows\n",
    "rows = [Row(login=key, location=value['location'] if value else None) for key, value in all_user_data.items()]\n",
    "\n",
    "# Create a Spark DataFrame from the Rows\n",
    "all_users_df = spark.createDataFrame(rows)\n",
    "all_users_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28b652e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `spark_df` is your Spark DataFrame\n",
    "pandas_df = all_users_df.toPandas()\n",
    "pandas_df.to_csv(\"data/user_location.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36bc37",
   "metadata": {},
   "source": [
    "#### 4. Select only the italian one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ed7a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|         login|    location|\n",
      "+--------------+------------+\n",
      "|    crondaemon|Turin, Italy|\n",
      "| CristianCosci|       Italy|\n",
      "|LudovicoCIocci|       Italy|\n",
      "|     FabTheZen| Bari, Italy|\n",
      "|     Manu098vm|       Italy|\n",
      "+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Assuming the list of Italian keywords is already defined\n",
    "italian_keywords = [\"rome\", \"milan\", \"italy\", \"florence\", \"venice\", \"naples\", \"turin\", \"palermo\", \"genoa\", \"bologna\"]\n",
    "\n",
    "# Convert the is_italian_location function to a PySpark UDF\n",
    "@udf(BooleanType())\n",
    "def is_italian_location_udf(location):\n",
    "    if not location:\n",
    "        return False\n",
    "    location = location.lower()\n",
    "    if any(keyword in location for keyword in italian_keywords):\n",
    "        return True\n",
    "    # Using fuzzy matching to account for typos\n",
    "    closest_match, score = process.extractOne(location, italian_keywords)\n",
    "    return score > 90\n",
    "\n",
    "# Filter in rows with Italian locations\n",
    "italian_df_spark = all_users_df.filter(is_italian_location_udf(col(\"location\")))\n",
    "\n",
    "# No need for flattening as there's no nested column 'actor'. Just display the result.\n",
    "italian_df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98ba19e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italian_df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc93d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      location|count|\n",
      "+--------------+-----+\n",
      "|         Italy|  168|\n",
      "|  Milan, Italy|   29|\n",
      "|   Rome, Italy|   20|\n",
      "|          Rome|   18|\n",
      "|         Milan|   13|\n",
      "|  Turin, Italy|   11|\n",
      "|        Milano|    8|\n",
      "|Bologna, Italy|    8|\n",
      "|         Turin|    6|\n",
      "| Verona, Italy|    6|\n",
      "|       Bologna|    5|\n",
      "|   Bari, Italy|    4|\n",
      "|Bergamo, Italy|    4|\n",
      "|Brescia, Italy|    4|\n",
      "|         italy|    3|\n",
      "|Palermo, Italy|    3|\n",
      "| Milano, Italy|    3|\n",
      "|     Milan, IT|    3|\n",
      "|    Venice, CA|    3|\n",
      "|Treviso, Italy|    3|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "italian_df_spark.groupBy(\"location\").agg(count(\"location\").alias(\"count\")).orderBy(\"count\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a233cd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('actor', StructType([StructField('avatar_url', StringType(), True), StructField('display_login', StringType(), True), StructField('gravatar_id', StringType(), True), StructField('id', LongType(), True), StructField('login', StringType(), True), StructField('url', StringType(), True)]), True)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select(\"actor\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceabebd",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f68ad419",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o473.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Unknown Source)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Unknown Source)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions$$Lambda$3689/5975950.apply$mcI$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3686/1030639414.apply(Unknown Source)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:572)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:522)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3672/1105860076.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$75/704106237.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3672/1105860076.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$75/704106237.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3672/1105860076.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$75/704106237.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3670/748390478.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3217/543278671.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3319/1897177885.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3218/125248680.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$3226/1208694368.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$3219/1317167135.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m filtered_actors_df \u001b[38;5;241m=\u001b[39m df_spark\u001b[38;5;241m.\u001b[39mjoin(italian_logins, df_spark\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mlogin \u001b[38;5;241m==\u001b[39m italian_logins\u001b[38;5;241m.\u001b[39mlogin, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mfiltered_actors_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\work_project\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\work_project\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\work_project\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\work_project\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o473.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Unknown Source)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Unknown Source)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions$$Lambda$3689/5975950.apply$mcI$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3686/1030639414.apply(Unknown Source)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:572)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:522)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3672/1105860076.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$75/704106237.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3672/1105860076.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$75/704106237.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3672/1105860076.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$75/704106237.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3670/748390478.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3217/543278671.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3319/1897177885.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3218/125248680.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$3226/1208694368.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$3219/1317167135.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Extract the 'login' column from the Italian users dataframe\n",
    "italian_logins = italian_df_spark.select(\"login\")\n",
    "\n",
    "# Perform an inner join to filter out the actors in df_spark based on the Italian logins\n",
    "filtered_actors_df = df_spark.join(italian_logins, df_spark.actor.login == italian_logins.login, 'inner').select(\"actor\")\n",
    "\n",
    "# Display the result\n",
    "filtered_actors_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e821cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
